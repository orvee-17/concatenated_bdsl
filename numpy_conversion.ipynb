{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"numpy_conversion.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p8hDi8NHyvEh","executionInfo":{"status":"ok","timestamp":1606582792368,"user_tz":-360,"elapsed":47577,"user":{"displayName":"Thasin Abedin ,160021139","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJQdb4tw5a_OefpLF2z3yEPD_pWAmIwHtBHC8i-A=s64","userId":"12939580268120638019"}},"outputId":"856033e2-3228-47c9-a414-71dab1c8c439"},"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)\n","import os\n","os.chdir('/content/drive/My Drive/Pose Estimation/orvee/Pretrained openpose')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RrBjc7YLybjh"},"source":["from __future__ import division\n","import os\n","import cv2\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from keras import Model, Sequential, utils\n","from keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n","from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.optimizers import Adam\n","from keras.models import load_model\n","\n","import pandas as pd \n","import numpy as np\n","from keras.preprocessing.image import ImageDataGenerator, load_img\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.image import img_to_array\n","from sklearn.preprocessing import LabelBinarizer\n","import matplotlib.pyplot as plt\n","from random import shuffle\n","import os\n","from keras.models import Model, Sequential, load_model\n","from keras import layers\n","from keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense, Activation,GlobalMaxPooling2D, ZeroPadding2D, concatenate, Input, Permute, Concatenate, BatchNormalization\n","from keras import applications\n","#from keras.utils import plot_model, to_categorical\n","from keras.applications import VGG16, ResNet50\n","from keras.optimizers import RMSprop,Adam  \n","from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n","import pickle\n","import random\n","import cv2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GnF7uLEoybjj"},"source":["def get_hand_model(modelpath):   #modelpath = hand_models\n","\n","    prototxt   = os.path.join(modelpath, \"pose_deploy.prototxt\")\n","    caffemodel = os.path.join(modelpath, \"pose_iter_102000.caffemodel\")\n","    hand_model = cv2.dnn.readNetFromCaffe(prototxt, caffemodel)\n","\n","    return hand_model\n","\n","num_points = 21\n","point_pairs = [[0,1],[1,2],[2,3],[3,4],\n","                    [0,5],[5,6],[6,7],[7,8],\n","                    [0,9],[9,10],[10,11],[11,12],\n","                    [0,13],[13,14],[14,15],[15,16],\n","                    [0,17],[17,18],[18,19],[19,20]]\n","#inWidth = 64\n","inHeight = 368\n","threshold = 0.1\n","modelpath = \"hand_models\"\n","hand_net = get_hand_model(modelpath)\n","\n","def pose_predict(imgfile):\n","        img_cv2 = cv2.imread(imgfile)\n","        # img_height, img_width, _ = img_cv2.shape\n","        img_height, img_width, _ = 224, 224, 1\n","        aspect_ratio = img_width / img_height\n","\n","        inWidth = int(((aspect_ratio * inHeight) * 8) // 8)\n","        inpBlob = cv2.dnn.blobFromImage(img_cv2, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)\n","\n","        hand_net.setInput(inpBlob)\n","\n","        output = hand_net.forward()\n","\n","        # # vis heatmaps\n","        # vis_heatmaps(imgfile, output)\n","\n","        #\n","        points = []\n","        for idx in range(num_points):\n","            probMap = output[0, idx, :, :] # confidence map.\n","            probMap = cv2.resize(probMap, (img_width, img_height))\n","\n","            # Find global maxima of the probMap.\n","            minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n","\n","            if prob > threshold:\n","                points.append([int(point[0]), int(point[1])])\n","            else:\n","                points.append([0, 0])\n","\n","        return points"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":884},"id":"GnFMMd8kybjj","executionInfo":{"status":"error","timestamp":1606593788350,"user_tz":-360,"elapsed":9409102,"user":{"displayName":"Thasin Abedin ,160021139","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJQdb4tw5a_OefpLF2z3yEPD_pWAmIwHtBHC8i-A=s64","userId":"12939580268120638019"}},"outputId":"8d956b48-eeae-4ba3-e02c-2a79da114993"},"source":["#################TO MAKE NPY FILE FROM THE DATASET, JUST CHANGE path to train and test directory one by one\n","imgs_path = \"bangla_dataset/RESIZED_TESTING_DATA/\"\n","img_files = os.listdir(imgs_path)\n","modelpath = \"hand_models\"\n","pose_list =[]\n","img_list = []\n","label_list = []\n","\n","\n","#print(img_files)\n","for img_file in img_files:\n","  pictures_dir = os.path.join(imgs_path, img_file)\n","  print(pictures_dir)\n","  pictures = [os.path.join(pictures_dir, pic) for pic in os.listdir(pictures_dir)]\n","  for pic in pictures:\n","    # img = np.array(Image.open(pic).resize((64,64)))\n","    # img_list.append(img)\n","    img = cv2.imread(pic,0)\n","    pic_pose = cv2.resize(img, (224, 224))\n","    img = np.array(pic_pose)\n","    img = img.astype('float32')/255\n","    img_list.append(img)\n","    res_points = pose_predict(pic)\n","    pose_list.append(res_points)\n","    label = tf.keras.utils.to_categorical(img_file, num_classes=38)\n","    label_list.append(label)            \n","print(\"[INFO]Done.\")\n"," "],"execution_count":null,"outputs":[{"output_type":"stream","text":["bangla_dataset/RESIZED_TESTING_DATA/0\n","bangla_dataset/RESIZED_TESTING_DATA/1\n","bangla_dataset/RESIZED_TESTING_DATA/10\n","bangla_dataset/RESIZED_TESTING_DATA/11\n","bangla_dataset/RESIZED_TESTING_DATA/12\n","bangla_dataset/RESIZED_TESTING_DATA/13\n","bangla_dataset/RESIZED_TESTING_DATA/14\n","bangla_dataset/RESIZED_TESTING_DATA/15\n","bangla_dataset/RESIZED_TESTING_DATA/16\n","bangla_dataset/RESIZED_TESTING_DATA/17\n","bangla_dataset/RESIZED_TESTING_DATA/18\n","bangla_dataset/RESIZED_TESTING_DATA/19\n","bangla_dataset/RESIZED_TESTING_DATA/2\n","bangla_dataset/RESIZED_TESTING_DATA/20\n","bangla_dataset/RESIZED_TESTING_DATA/21\n","bangla_dataset/RESIZED_TESTING_DATA/22\n","bangla_dataset/RESIZED_TESTING_DATA/23\n","bangla_dataset/RESIZED_TESTING_DATA/24\n","bangla_dataset/RESIZED_TESTING_DATA/25\n","bangla_dataset/RESIZED_TESTING_DATA/26\n","bangla_dataset/RESIZED_TESTING_DATA/27\n","bangla_dataset/RESIZED_TESTING_DATA/28\n","bangla_dataset/RESIZED_TESTING_DATA/29\n","bangla_dataset/RESIZED_TESTING_DATA/3\n","bangla_dataset/RESIZED_TESTING_DATA/30\n","bangla_dataset/RESIZED_TESTING_DATA/31\n","bangla_dataset/RESIZED_TESTING_DATA/32\n","bangla_dataset/RESIZED_TESTING_DATA/33\n","bangla_dataset/RESIZED_TESTING_DATA/34\n","bangla_dataset/RESIZED_TESTING_DATA/35\n","bangla_dataset/RESIZED_TESTING_DATA/36\n","bangla_dataset/RESIZED_TESTING_DATA/37\n","bangla_dataset/RESIZED_TESTING_DATA/4\n","bangla_dataset/RESIZED_TESTING_DATA/5\n","bangla_dataset/RESIZED_TESTING_DATA/6\n","bangla_dataset/RESIZED_TESTING_DATA/7\n","bangla_dataset/RESIZED_TESTING_DATA/8\n","bangla_dataset/RESIZED_TESTING_DATA/9\n","bangla_dataset/RESIZED_TESTING_DATA/imglist.npy\n"],"name":"stdout"},{"output_type":"error","ename":"NotADirectoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-34603ef0ac8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mpictures_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpictures_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mpictures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpictures_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpictures_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpictures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# img = np.array(Image.open(pic).resize((64,64)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'bangla_dataset/RESIZED_TESTING_DATA/imglist.npy'"]}]},{"cell_type":"code","metadata":{"id":"xsoImqSwybjl"},"source":["#Saving the training numpy files to feed to the main neural network. Similarly do it for test images.\n","np.save(\"bangla_dataset/np_files/imglist_t.npy\", img_list)   #npy contains (no. of images, img_length, img_breadth, no. of channel)\n","np.save(\"bangla_dataset/np_files/poselist_t.npy\", pose_list)   #npy contains (no. of images, no. of points = 22)\n","np.save(\"bangla_dataset/np_files/labellist_t.npy\", label_list)  #npy contains (no. of images, no. of labels = 38)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mRax-YPqHYf","executionInfo":{"status":"ok","timestamp":1606597356284,"user_tz":-360,"elapsed":1198,"user":{"displayName":"Thasin Abedin ,160021139","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJQdb4tw5a_OefpLF2z3yEPD_pWAmIwHtBHC8i-A=s64","userId":"12939580268120638019"}},"outputId":"104d2d73-8960-4be5-a5c6-d2b5d28dfc15"},"source":["#Example of loading a numpy file for use\n","a = np.load(\"bangla_dataset/np_files/labellist_t.npy\", allow_pickle= True)\n","a.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1520, 38)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"5xO7TYWcqREJ"},"source":[""],"execution_count":null,"outputs":[]}]}